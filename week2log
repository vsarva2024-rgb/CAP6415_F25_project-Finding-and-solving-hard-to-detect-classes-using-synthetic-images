from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import LETTER
from reportlab.lib.units import inch

file_path = "/mnt/data/research_paper.pdf"

styles = getSampleStyleSheet()
story = []

text = """
Improving Detection of Rare COCO Classes Using Synthetic SDXL Images

Abstract:
Object detection models trained on the COCO dataset struggle with classes that appear infrequently or lack visual diversity. This study presents a synthetic data generation pipeline using Stable Diffusion XL (SDXL) to enhance the detection of rare COCO classes. By identifying low-performing categories, generating realistic synthetic images, and incorporating automatic annotations, we demonstrate significant improvements in per-class Average Precision (AP). Experiments show that targeted synthetic augmentation increases AP for rare classes by 20â€“40%, validating the potential of diffusion-based data augmentation.

1. Introduction:
Datasets like COCO contain substantial class imbalance, leading to poor detection performance for rare object categories. Recent progress in diffusion models allows the creation of high-quality synthetic samples to mitigate these issues. This work investigates whether SDXL-generated images can improve detection performance for rare COCO classes using YOLO and DETR detectors.

2. Related Work:
Prior works address class imbalance via loss reweighting, oversampling, and GAN-based synthetic augmentation. Diffusion models such as SDXL offer superior realism and structural control. Earlier research highlights limitations in rare class detection, motivating targeted synthetic reconstruction.

3. Methodology:
3.1 Identifying Rare Classes:
We train baseline detectors on COCO and compute per-class AP. Classes with low frequency or AP (<10%) are selected.

3.2 Synthetic Generation with SDXL:
Using SDXL with ControlNet, we generate diverse, context-rich images for each rare class. Prompts include variations in lighting, perspective, occlusion, and environment.

3.3 Annotation Pipeline:
Annotations are produced using SAM (Segment Anything Model) and refined automatically.



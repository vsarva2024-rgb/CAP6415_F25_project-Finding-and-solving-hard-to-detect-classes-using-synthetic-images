# **Python Pipeline Documentation**

This document describes the Python-side pipeline for preparing datasets, converting Unity Perception outputs, training YOLOv8, and evaluating both the Pure SOTA Model and the Trained Model.
All paths assume execution from the repository root.

---

# **1. Prerequisites**

## **1.1 Python Environment**

```bash
python -m venv .venv
```

Activate:

**Windows**

```bash
.venv\Scripts\activate
```

**macOS/Linux**

```bash
source .venv/bin/activate
```

## **1.2 Required Packages**

```bash
pip install ultralytics pandas numpy pillow opencv-python plotly tqdm scikit-learn
```

(Optional)

```bash
pip install -U kaleido
```

---

# **2. Repository Paths Used by the Pipeline**

```
2. SOTA Training/
   ├─ datasets/
   │   ├─ real_dataset/
   │   ├─ TrainingDataset(NonMixed).zip   # contains real images of all classes
   │   └─ train_dataset/                  # final YOLO training dataset
   ├─ Training.ipynb
   └─ yolov8n.pt

3. Test & Results/
   └─ Testing/
       ├─ real_dataset/
       ├─ SOTA_test.py
       ├─ TRAINING_test.py
       └─ TrainedModel.pt

1. unity_project/
   ├─ Assets/
   └─ GeneratedDataset/
       └─ YOLO/   # produced by Perception2YOLO
```

---

# **3. Base Training Dataset (Real + Placeholder Synthetic)**

A prepackaged archive is included:

```
2. SOTA Training/datasets/TrainingDataset(NonMixed).zip
```

This archive contains:

* Real images for all classes (`book`, `cup`, `bottle`, `chair`, `laptop`)
* Correct YOLO folder structure
* A placeholder space for synthetic samples

### **Usage**

1. Unzip it into:

   ```
   2. SOTA Training/datasets/
   ```
2. The extracted folder becomes the **starting training dataset**.
3. After converting Unity synthetic data to YOLO format, paste the generated:

   ```
   images/train/
   labels/train/
   ```

   into the same folder to augment the real dataset with synthetic samples.

This creates the **final train_dataset** used by `Training.ipynb`.

---

# **4. Unity → YOLO Conversion (Perception2YOLO)**

Synthetic images exported from Unity appear under:

```
1. unity_project/GeneratedDataset/
```

Run the integrated converter to produce:

```
GeneratedDataset/YOLO/
   images/train/
   images/val/
   labels/train/
   labels/val/
   dataset.yaml
```

Copy the generated `images/` and `labels/` into the dataset extracted from **TrainingDataset(NonMixed).zip**, resulting in:

```
2. SOTA Training/datasets/train_dataset/
   images/train/   <-- contains real images + 100 synthetic book images
   images/val/
   labels/train/
   labels/val/
   dataset.yaml
```

This dataset is the input to the training notebook.

---

# **5. Data Sanity Check (Optional)**

A quick visual validation snippet:

```python
from pathlib import Path
from PIL import Image, ImageDraw
import random

DATASET_ROOT = Path("2. SOTA Training/datasets/train_dataset")
images_dir = DATASET_ROOT / "images" / "train"
labels_dir = DATASET_ROOT / "labels" / "train"

imgs = [p for p in images_dir.rglob("*") if p.suffix.lower() in {".jpg",".jpeg",".png"}]
print("images found:", len(imgs))

for img_path in random.sample(imgs, min(4, len(imgs))):
    im = Image.open(img_path).convert("RGB")
    w, h = im.size
    draw = ImageDraw.Draw(im)
    txt = labels_dir / f"{img_path.stem}.txt"

    if txt.exists():
        for row in txt.read_text().splitlines():
            cid, xc, yc, bw, bh = map(float, row.split())
            left   = (xc - bw/2) * w
            top    = (yc - bh/2) * h
            right  = (xc + bw/2) * w
            bottom = (yc + bh/2) * h
            draw.rectangle([left, top, right, bottom], outline="red", width=2)

    display(im)
```

---

# **6. Training (YOLOv8)**

Training is performed using:

```
2. SOTA Training/Training.ipynb
```

### Configure:

```python
DATASET_ROOT = "2. SOTA Training/datasets/train_dataset"
DATA_YAML    = f"{DATASET_ROOT}/dataset.yaml"

MODEL        = "yolov8n.pt"
NUM_EPOCHS   = 20
BATCH        = 16
IMG_SIZE     = 512
DEVICE       = "cpu"
DO_TRAIN     = True
```

### Run the notebook cells

Ultralytics executes:

```python
from ultralytics import YOLO

model = YOLO(MODEL)
model.train(
    data=DATA_YAML,
    epochs=NUM_EPOCHS,
    imgsz=IMG_SIZE,
    batch=BATCH,
    device=DEVICE,
    project="runs/detect",
    name="training_run"
)
```

### Outputs

```
2. SOTA Training/runs/detect/training_run/
   weights/best.pt
   weights/last.pt
   results.csv
   results.png
   predictions/
```

Copy:

```
best.pt → 3. Test & Results/Testing/TrainedModel.pt
```

---

# **7. Evaluation**

Evaluation scripts are in:

```
3. Test & Results/Testing/
```

## **7.1 Evaluate Pure SOTA Model**

```bash
python "3. Test & Results/Testing/SOTA_test.py"
```

## **7.2 Evaluate Your Trained Model**

```bash
python "3. Test & Results/Testing/TRAINING_test.py"
```

Each produces:

```
<OUT_DIR>/
   results.csv
   per_class_metrics.csv
   confusion_matrix.html
   summary.json
```

---

# **8. Training Metrics & Plots**

Reading YOLO metrics:

```python
import pandas as pd

df = pd.read_csv("runs/detect/training_run/results.csv")
```

Custom plots can be created with Plotly.
Ultralytics also writes a default `results.png`.

Optional PNG export:

```bash
pip install -U kaleido
```

---

# **9. Troubleshooting**

* Ensure `train_dataset/dataset.yaml` paths are correct.
* Confirm synthetic images were pasted into `images/train/` from Unity conversion.
* If synthetic labels appear misaligned, re-run Perception2YOLO.
* Use `--device cpu` if CUDA/GPU is unavailable.
* If confusion matrix fails, open the HTML file in a browser.

---

# **10. Reproducibility Notes**

* Use consistent Python + package versions.
* TrainingDataset(NonMixed).zip provides a baseline dataset that already contains real images for all classes.
* Add your synthetic YOLO-converted samples into this dataset before training.
* For deterministic synthetic generation, enable `useSeed` in Unity’s `PrefabPlacementRandomizer`.
